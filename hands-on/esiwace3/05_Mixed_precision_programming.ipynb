{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7AowbuXIDRdM"
   },
   "source": [
    "# GPU Optimization with Kernel Tuner\n",
    "\n",
    "This notebook shows an example of how we can use mixed precision to improve the performance and energy efficiency of our GPU code. We'll again use Kernel Tuner to benchmark different code variants and Kernel Float for simplified mixed-precision CUDA programming.\n",
    "\n",
    "### Dependencies\n",
    "First, we need to import the necessary packages into python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JigCjyqQ4RnC"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import seaborn\n",
    "import scipy.signal.windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imihZESFDuaV"
   },
   "source": [
    "Next, we need to download Kernel Float. Since `kernel_float` is a header-only library, the simplest method of using it is to download the `single_include` version of the library, which downloads just a single C++ header file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s_OG1_f84fZB"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!wget -O kernel_float.h https://raw.githubusercontent.com/KernelTuner/kernel_float/v0.2/single_include/kernel_float.h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kugaq3kREIAw"
   },
   "source": [
    "The next cell will write the CUDA kernel that we will be tuning to `convolution.cu`. The kernel is a simple 1D convolution kernel.\n",
    "\n",
    "It has the following tunable parameters:\n",
    "* Data types:\n",
    " * `INPUT_TYPE`: Type of input data.\n",
    " * `OUTPUT_TYPE`: Type of output data.\n",
    " * `FILTER_TYPE`: Type of filter.\n",
    " * `ACCUM_TYPE`: Type used inside kernel for the accumulation.\n",
    "* Parameters:\n",
    " * `block_size_x`: Threads per block.\n",
    " * `VECTOR_SIZE`: Number of elements assigned to each thread.\n",
    " * `PREFETCH_INPUT`: Whether to use shared memory to prefetch inputs.\n",
    " * `loop_unroll_factor_filter`: How much to unroll the inner loop.\n",
    "\n",
    "While you can take a look at the code, there is no need to modify it or fully understand the details for this hands-on session.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S5vFF9vy4wvR"
   },
   "outputs": [],
   "source": [
    "%%writefile convolution.cu\n",
    "\n",
    "#include \"kernel_float.h\"\n",
    "\n",
    "using bfloat16 = __nv_bfloat16;\n",
    "using half = __nv_half;\n",
    "\n",
    "extern \"C\"\n",
    "__launch_bounds__(block_size_x)\n",
    "__global__ void convolution(\n",
    "        OUTPUT_TYPE* __restrict__ output,\n",
    "  const INPUT_TYPE*  __restrict__ input,\n",
    "  const FILTER_TYPE* __restrict__ filter\n",
    ") {\n",
    "  const int total_block_size = block_size_x * VECTOR_SIZE;\n",
    "  const int thread_x = threadIdx.x * VECTOR_SIZE;\n",
    "  const int global_x = blockIdx.x * total_block_size + thread_x;\n",
    "\n",
    "    // If we prefetch, read all required input elements into shared memory.\n",
    "#if PREFETCH_INPUT\n",
    "    const int shared_input_size = total_block_size + FILTER_SIZE - 1;\n",
    "    __shared__ INPUT_TYPE shared_input[shared_input_size];\n",
    "\n",
    "    for (int dx = 0; dx < shared_input_size; dx += total_block_size) {\n",
    "      if (global_x + dx < INPUT_SIZE && thread_x + dx < shared_input_size) {\n",
    "        auto items = kernel_float::read_aligned<VECTOR_SIZE>(\n",
    "          &input[global_x + dx]\n",
    "        );\n",
    "\n",
    "        kernel_float::write_aligned<VECTOR_SIZE>(\n",
    "          &shared_input[thread_x + dx], items\n",
    "        );\n",
    "      }\n",
    "    }\n",
    "\n",
    "    __syncthreads();\n",
    "#endif\n",
    "\n",
    "  if (global_x < OUTPUT_SIZE) {\n",
    "    kernel_float::vec<ACCUM_TYPE, VECTOR_SIZE> accumulate;\n",
    "\n",
    "#pragma unroll loop_unroll_factor_filter\n",
    "    for (int dx = 0; dx < FILTER_SIZE; dx++) {\n",
    "\n",
    "      // Read the input at dx, either from shared memory or global memory.\n",
    "#if PREFETCH_INPUT\n",
    "      auto items = kernel_float::read<VECTOR_SIZE>(&shared_input[thread_x + dx]);\n",
    "#else\n",
    "      auto items = kernel_float::read<VECTOR_SIZE>(&input[global_x + dx]);\n",
    "#endif\n",
    "\n",
    "      // Read the weight at dx\n",
    "      FILTER_TYPE weight = filter[dx];\n",
    "\n",
    "      // Add the product of the items multiplied by the weights\n",
    "      // `fma` is fused-mulitply-add, it performs \"weight * items + accumulate\"\n",
    "      accumulate = kernel_float::fma(\n",
    "          kernel_float::cast<ACCUM_TYPE>(weight),\n",
    "          kernel_float::cast<ACCUM_TYPE>(items),\n",
    "          accumulate\n",
    "      );\n",
    "    }\n",
    "\n",
    "    // Write the result to the output\n",
    "    kernel_float::write_aligned<VECTOR_SIZE>(&output[global_x], accumulate);\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x7a0o11HFFts"
   },
   "source": [
    "### Problem dimensions\n",
    "\n",
    "Now we generate inputs by defining the `filter_size` and the `problem_size`. The output signal will consist of `problem_size` samples and the filter will have size `filter_size`.\n",
    "\n",
    "Note that the size of the input signal is calculated automatically from the size of the output signal and the filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C7wN0Eo045TW"
   },
   "outputs": [],
   "source": [
    "filter_size = 25\n",
    "problem_size = 50_000_000\n",
    "\n",
    "\n",
    "results = []\n",
    "input = np.random.default_rng(0).random(problem_size + filter_size - 1)\n",
    "\n",
    "filter = scipy.signal.windows.gaussian(filter_size, 2.0)\n",
    "filter /= np.sum(filter)\n",
    "\n",
    "output = np.zeros(problem_size)\n",
    "output_expect = np.convolve(input, filter, mode=\"valid\")\n",
    "\n",
    "plt.plot(np.arange(100), input[:100], label=\"input\")\n",
    "plt.plot(np.arange(100) + filter_size/2, output_expect[:100], label=\"expected output\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TcFqJlOvFqbY"
   },
   "source": [
    "### Auto-tuning using Kernel Tuner\n",
    "\n",
    "The next cell shows a simple example of how to perform accuracy tuning using Kernel Tuner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WsBGYMdn440C"
   },
   "outputs": [],
   "source": [
    "import kernel_tuner\n",
    "from kernel_tuner.accuracy import TunablePrecision, AccuracyObserver\n",
    "from kernel_tuner.observers.nvml import NVMLObserver\n",
    "\n",
    "# The tunable types. Currently, the code only tunes for double precision.\n",
    "#\n",
    "# ================================================================\n",
    "# ✏️ Add you own data types here. You can use the following options: ✏️\n",
    "#\n",
    "# - \"double\"\n",
    "# - \"float\"\n",
    "# - \"half\"\n",
    "# - \"bfloat16\"\n",
    "# ================================================================\n",
    "#\n",
    "tune_params = dict()\n",
    "tune_params[\"OUTPUT_TYPE\"] = [\"double\"]\n",
    "tune_params[\"INPUT_TYPE\"] = [\"double\"]\n",
    "tune_params[\"FILTER_TYPE\"] = [\"double\"]\n",
    "tune_params[\"ACCUM_TYPE\"] = [\"double\"]\n",
    "\n",
    "# Other tunable parameters\n",
    "tune_params[\"block_size_x\"] = [128, 256]\n",
    "tune_params[\"loop_unroll_factor_filter\"] = [1, 5, 25]\n",
    "tune_params[\"VECTOR_SIZE\"] = [1, 2, 4]\n",
    "tune_params[\"PREFETCH_INPUT\"] = [0, 1]\n",
    "\n",
    "\n",
    "# Do not modify this cell below this line\n",
    "# =======================================\n",
    "\n",
    "# Kernel arguments wrapped in `TunablePrecision`s\n",
    "args = [\n",
    "    TunablePrecision(\"OUTPUT_TYPE\", output),\n",
    "    TunablePrecision(\"INPUT_TYPE\", input),\n",
    "    TunablePrecision(\"FILTER_TYPE\", filter),\n",
    "]\n",
    "\n",
    "# The expected output\n",
    "answer = [output_expect, None, None]\n",
    "\n",
    "# Add observers to measure the accuracy and energy\n",
    "observers = [AccuracyObserver(\"RMSE\"), NVMLObserver([\"nvml_energy\", \"nvml_power\"])]\n",
    "\n",
    "# Tune for 2 minutes using random sampling\n",
    "strategy = \"random_sample\"\n",
    "strategy_options = dict(time_limit=2 * 60, fraction=1)\n",
    "\n",
    "# These compiler options are needed for compilation.\n",
    "compiler_options=[\n",
    "    \"--std=c++17\",\n",
    "    \"-I/content\",\n",
    "    \"-I/usr/local/cuda/include\",\n",
    "    f\"-DINPUT_SIZE={len(input)}\",\n",
    "    f\"-DOUTPUT_SIZE={len(output)}\",\n",
    "    f\"-DFILTER_SIZE={len(filter)}\",\n",
    "]\n",
    "\n",
    "# There are no restrictions for this kernel\n",
    "restriction = []\n",
    "\n",
    "# Run kernel tuner!\n",
    "results_batch, env = kernel_tuner.tune_kernel(\n",
    "    \"convolution\",\n",
    "    \"convolution.cu\",\n",
    "    problem_size,\n",
    "    args,\n",
    "    tune_params,\n",
    "    compiler_options=compiler_options,\n",
    "    answer=answer,\n",
    "    observers=observers,\n",
    "    lang=\"NVCUDA\",\n",
    "    strategy=strategy,\n",
    "    strategy_options=strategy_options,\n",
    "    iterations=15,\n",
    "    restrictions=restriction,\n",
    "    cache=\"convolution\"\n",
    ")\n",
    "\n",
    "results += results_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4b0By89hTeg4"
   },
   "source": [
    "### Analyzing the Results\n",
    "\n",
    "We can convert the results into a Pandas dataframe to visualize them using a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iiRicf3IPzQ0"
   },
   "outputs": [],
   "source": [
    "# Convert to DataFrame and remove some unnecessary columns\n",
    "remove_columns = [\"times\", \"timestamp\", \"compile_time\", \"strategy_time\", \"benchmark_time\", \"verification_time\", \"framework_time\"]\n",
    "df = pandas.DataFrame(results).drop(columns=remove_columns).sort_values(\"time\")\n",
    "\n",
    "# Find speedup over lowest error, which should be double precision\n",
    "baseline_time = 10.605600 #df[\"time\"][df[\"error\"] == df[\"error\"].min()].min()\n",
    "df[\"speedup\"] = baseline_time / df[\"time\"]\n",
    "\n",
    "# Add column that indicates used data types\n",
    "type_columns = [\"OUTPUT_TYPE\", \"INPUT_TYPE\", \"FILTER_TYPE\", \"ACCUM_TYPE\"]\n",
    "df[\"data_types\"] = df[type_columns].apply(lambda row: \"+\".join(sorted(set(row))), axis=1)\n",
    "\n",
    "# Show table\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LMFnVVVjT883"
   },
   "source": [
    "We can visualize the results on a performance vs error plot. We group them based on the data types used. For example, `half+double` means the data types used are a combination of `double` and `half`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Evi8NP-k8BZJ"
   },
   "outputs": [],
   "source": [
    "plt.subplots(1, 2, figsize=(15, 8))\n",
    "\n",
    "# Speedup versus error\n",
    "plt.subplot(121)\n",
    "plt.title(\"Speedup versus error\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Speedup over double precision\")\n",
    "plt.ylabel(\"Error (RMSE)\")\n",
    "seaborn.scatterplot(df, x=\"speedup\", y=\"error\", hue=\"data_types\")\n",
    "\n",
    "# Energy versus error\n",
    "plt.subplot(122)\n",
    "plt.title(\"Energy versus error\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Energy usage (J)\")\n",
    "plt.ylabel(\"Error (RMSE)\")\n",
    "seaborn.scatterplot(df, x=\"nvml_energy\", y=\"error\", hue=\"data_types\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F26ZRkCkVXBZ"
   },
   "source": [
    "### Pareto front\n",
    "\n",
    "Next we determine the _pareto front_. The pareto front is the set of all configurations that are _optimal_ in the sense that it is not possible to improve one property (the error or the run time or the energy) without sacrificing some other property.\n",
    "\n",
    "First, we print the pareto optimal points as a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8K08sy4HHIF_"
   },
   "outputs": [],
   "source": [
    "# Function that checks if `row` dominates all other rows\n",
    "def dominates(row):\n",
    "    dominators = (row.time >= df[\"time\"]) & (row.error >= df[\"error\"]) & (row.nvml_energy >= df[\"nvml_energy\"]) & \\\n",
    "              ((row.time > df[\"time\"]) | (row.error > df[\"error\"]) | (row.nvml_energy > df[\"nvml_energy\"]))\n",
    "\n",
    "    return not dominators.any()\n",
    "\n",
    "# Extract the pareto front\n",
    "df[\"pareto\"] = df.apply(dominates, axis=1)\n",
    "\n",
    "df.loc[df[\"pareto\"]].sort_values(\"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5PBuwt6ZTCJ9"
   },
   "source": [
    "We can also visualize the front on a speedup-error scatter plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "51bBIJVZN9wa"
   },
   "outputs": [],
   "source": [
    "# Speedup versus error\n",
    "plt.title(\"Speedup versus error\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Speedup over double precision\")\n",
    "plt.ylabel(\"Error (RMSE)\")\n",
    "\n",
    "seaborn.scatterplot(df.sort_values(\"pareto\"), x=\"speedup\", y=\"error\", hue=\"pareto\", palette=[\"lightgray\", \"red\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eC4_qd91TOHV"
   },
   "source": [
    "## Go experiment!\n",
    "Now, go back up to the fifth cell of this notebook, change some of the data types in `tune_params`, and execute the cell again. Afterwards, you can rerun the subsequent cells to see new results for different combinations of types.\n",
    "\n",
    "\n",
    "**That's it! You've successfully completed the second hands-on of the day!**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
