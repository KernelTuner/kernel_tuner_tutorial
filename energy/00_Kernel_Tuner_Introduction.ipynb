{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AmH1n4971xm0"
   },
   "source": [
    "# Kernel Tuner Tutorial\n",
    "\n",
    "## Introduction Hands-on\n",
    "\n",
    "Welcome to the first hands-on of the Kernel Tuner tutorial. In this hands-on exercise we will learn how to **install** and access Kernel Tuner from Python, and **tune** our first CUDA kernel.\n",
    "\n",
    "To install the latest version of `kernel_tuner` we use `pip`, but before installing it we need to have a working CUDA installation available, and be sure to have both `numpy`  and `cupy` installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7LBBP5n8EV-H",
    "outputId": "b4957bae-df20-47cc-9382-4a5b2f143f7f"
   },
   "outputs": [],
   "source": [
    "%pip install kernel-tuner[tutorial]==1.0.0b3\n",
    "%pip install nvidia-ml-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cR2rHfIa4e7f"
   },
   "source": [
    "After installing all necessary packages, we can import `numpy` and `kernel_tuner`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "As8FyYq3EtoK"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import kernel_tuner as kt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MLe_BXzR5PTk"
   },
   "source": [
    "Before using Kernel Tuner, we will create a text file containing the code of the CUDA kernel that we are going to use in this hands-on.\n",
    "\n",
    "This simple kernel is called `vector_add` and computes the elementwise sum of two vectors of size `n`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xgnQk_bmEx_F",
    "outputId": "b25819f4-9e0a-43a4-dd79-7f5de694d824"
   },
   "outputs": [],
   "source": [
    "%%writefile vector_add_kernel.cu\n",
    "\n",
    "__global__ void vector_add(float * c, float * a, float * b, int n) {\n",
    "    int i = (blockIdx.x * blockDim.x) + threadIdx.x;\n",
    "\n",
    "    if ( i < n ) {\n",
    "        c[i] = a[i] + b[i];\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cbkMDIgO7V9h"
   },
   "source": [
    "The execution of the cell above created the file `vector_add_kernel.cu` containing the source code of our kernel.\n",
    "\n",
    "We can now use Kernel Tuner to execute the code on the GPU; please read carefully both code and comments to become familiar with how Kernel Tuner works.\n",
    "\n",
    "For more details refer to the [API](https://KernelTuner.github.io/kernel_tuner/stable/user-api.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cZPTL1kjE-OS",
    "outputId": "720a6332-41fd-4db2-c5df-3230413aca4e"
   },
   "outputs": [],
   "source": [
    "# the size of the vectors\n",
    "size = 1000000\n",
    "\n",
    "# all the kernel input and output data need to use numpy data types,\n",
    "# note that we explicitly state that these arrays should consist of\n",
    "# 32 bit floating-point values, to match our kernel source code\n",
    "a = np.random.randn(size).astype(np.float32)\n",
    "b = np.random.randn(size).astype(np.float32)\n",
    "c = np.zeros_like(b)\n",
    "n = np.int32(size)\n",
    "\n",
    "# now we combine these variables in an argument list, which matches\n",
    "# the order and types of the function arguments of our CUDA kernel\n",
    "args = [c, a, b, n]\n",
    "\n",
    "# the next step is to create a dictionary to tell Kernel Tuner about\n",
    "# the tunable parameters in our code and what values these may take\n",
    "tune_params = dict(block_size_x=[16, 32, 64, 128, 256, 512, 1024])\n",
    "\n",
    "# finally, we call tune_kernel to start the tuning process. To do so,\n",
    "# we pass\n",
    "#    the name of the kernel we'd like to tune, in our case: \"vector_add\",\n",
    "#    the name of the file containing our source code,\n",
    "#    the problem_size that our kernel operates on\n",
    "#    the argument list to call our kernel function\n",
    "#    the dictionary with tunable parameters\n",
    "results, env = kt.tune_kernel(\"vector_add\", \"vector_add_kernel.cu\", size, args, tune_params, lang=\"cupy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IF1HCWx4Dzp8"
   },
   "source": [
    "\n",
    "\n",
    "The `tune_kernel` function returns two outputs that we saved as `results` and `env`:\n",
    "\n",
    "* `results` is a list of dictionaries, each containing detailed information about the configurations that have been benchmarked;\n",
    "* `env` is a dictionary that stores information about the hardware and software environment in which this experiment took place; it is recommended to store this information along with the benchmark results.\n",
    "\n",
    "We can also print the content of `results` to have a look at the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LHlbScKUGir3",
    "outputId": "c52ab18b-aae6-4a03-9946-e75b35149617"
   },
   "outputs": [],
   "source": [
    "print(f\"Number of configurations: {len(results)}\")\n",
    "for res in results:\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NbvRO_gzFOcU"
   },
   "source": [
    "As we can see the results returned by `tune_kernel` lists the average execution time of our kernel in the field \"time\" and even includes the individual measurements of each kernel that was benchmarked stored under \"times\".\n",
    "\n",
    "Kernel Tuner also collects a lot of other timing information, including the overall time it took to compile our kernel and benchmark our kernel.\n",
    "\n",
    "## Energy measurements\n",
    "\n",
    "However, today we are interested in particular in the energy used by our kernels, so let's add our first **Observer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wu0TxViRHU28"
   },
   "outputs": [],
   "source": [
    "from kernel_tuner.observers.nvml import NVMLObserver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ynLlsP-VHU29"
   },
   "source": [
    "The NVMLObserver uses the Nvidia Management Library to query all kinds of information about our GPU while the kernel is running. We can measure many things, like the clock frequency, temperature, but also power usage of our GPU in this way.\n",
    "\n",
    "Let's setup an NVMLObserver and tell it about the quantities we want to observe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "id": "_ENrEKyrHU29",
    "outputId": "8a879cad-1ed3-49f4-c667-b0ae1402de56"
   },
   "outputs": [],
   "source": [
    "# Setup the NVMLObserver\n",
    "\n",
    "# among the options we can choose from are:\n",
    "# \"nvml_power\", \"nvml_energy\", \"core_freq\", \"mem_freq\", \"temperature\"\n",
    "\n",
    "# The constructor expects to receive a list of 'observerables', e.g. NVMLObserver([\"nvml_energy\", \"temperature\"])\n",
    "# Finish the code below to pick which quantities you want to observe while tuning and construct the NVMLObserver\n",
    "\n",
    "nvmlobserver = NVMLObserver([\"nvml_energy\", ...]) # TODO: replace ... with something else\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ohIyUwBMHU29"
   },
   "source": [
    "The quantities observed by the Observers are added to the results returned by `tune_kernel`. They are however, not directly printed while `tune_kernel` is running. User-defined metrics are always printed to screen, so let's add a few metrics as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cnYRa22GHU29",
    "outputId": "3e1b2cb3-6ef0-40c8-a460-037ec63aaad5"
   },
   "outputs": [],
   "source": [
    "nvmlobserver = NVMLObserver([\"nvml_energy\", \"temperature\"]) # remove this line for the tutorial\n",
    "\n",
    "metrics = dict()\n",
    "metrics[\"GFLOP/s\"]  = lambda p: (size/1e9) / (p[\"time\"]/1e3)  # Kernel Tuner's time is always in ms, so convert to s\n",
    "metrics[\"GFLOPS/W\"] = lambda p: (size/1e9) / p[\"nvml_energy\"] # computed as GFLOP/J\n",
    "# Optional TODO: add another metric\n",
    "#metrics[\"my_metric\"] = lambda p: p[...]\n",
    "\n",
    "results, env = kt.tune_kernel(\"vector_add\", \"vector_add_kernel.cu\", size, args, tune_params,\n",
    "                              observers=[nvmlobserver], metrics=metrics,\n",
    "                              cache=\"vec_add_cache.json\", lang=\"cupy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tQnkaCFyHU2-"
   },
   "source": [
    "Kernel Tuner has now printed for every kernel it benchmarked the the performance in GFLOP/s and the energy efficiency in GFLOPs/W. To get a better view of these performance results and understand how changing the thread block dimensions `block_size_x` influences the results, we can use Kernel Tuner's dashboard.\n",
    "\n",
    "Install and import dashboard by running the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 696
    },
    "id": "uuj04nI7HU2-",
    "outputId": "1d835016-e6a7-4a76-8345-cb99ba7ed8e6"
   },
   "outputs": [],
   "source": [
    "# The following two lines are a workaround for an issue with Google Colab\n",
    "# these are not needed when you run this locally\n",
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "\n",
    "# install and import Kernel Tuner dashboard\n",
    "%pip install git+https://github.com/KernelTuner/dashboard\n",
    "import panel as pn\n",
    "pn.extension(comms='colab')\n",
    "import ktdashboard.ktdashboard as ktd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "glt38MjsHU2-"
   },
   "source": [
    "Now, let's view the results from our last run, by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "id": "TNZsQ1uBHU2-",
    "outputId": "794dbbbe-5a2f-4d72-c8f6-a43df426b8d2"
   },
   "outputs": [],
   "source": [
    "ktd.KTdashboard(\"vec_add_cache.json\").notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qTDTOi9wJvX-"
   },
   "source": [
    "The visualization above is interactive, you can view the relationship between the different results obtained with Kernel Tuner. For example, try to change the x-axis to GFLOPS/W or block_size_x. You can also hover over the points to inspect which kernel configuration this point represents.\n",
    "\n",
    "**That's it! You've successfully completed the first hands-on!**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "00-Kernel_Tuner-Introduction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
