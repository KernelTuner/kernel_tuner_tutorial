{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Mixed Precision Auto-tuning\n",
        "\n",
        "This notebook shows an example of how to perform mixed precision auto-tuning using Kernel Tuner and Kernel Float.\n",
        "\n",
        "## Dependencies\n",
        "First, we need to install `kernel_tuner`, `cuda-python`, and `pynvml` from `pip`. Afer this, we can import the necessary packages into python."
      ],
      "metadata": {
        "id": "7AowbuXIDRdM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JigCjyqQ4RnC"
      },
      "outputs": [],
      "source": [
        "!pip install kernel_tuner==1.0.0b3 cuda-python==11.8 pynvml seaborn==0.13\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas\n",
        "import seaborn\n",
        "import scipy.signal.windows"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we need to download Kernel Float. Since `kernel_float` is a header-only library, the simplest method of using it is to download the `single_include` version of the library, which downloads just a single C++ header file."
      ],
      "metadata": {
        "id": "imihZESFDuaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O kernel_float.h https://raw.githubusercontent.com/KernelTuner/kernel_float/v0.2/single_include/kernel_float.h"
      ],
      "metadata": {
        "id": "s_OG1_f84fZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next cell will write the CUDA kernel that we will be tuning to `convolution.cu`. The kernel is a simple convolution kernel. You can read through the code to understand its purpose."
      ],
      "metadata": {
        "id": "Kugaq3kREIAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile convolution.cu\n",
        "\n",
        "#include \"kernel_float.h\"\n",
        "\n",
        "using bfloat16 = __nv_bfloat16;\n",
        "using half = __nv_half;\n",
        "\n",
        "extern \"C\"\n",
        "__launch_bounds__(block_size_x)\n",
        "__global__ void convolution(\n",
        "        OUTPUT_TYPE* __restrict__ output,\n",
        "  const INPUT_TYPE*  __restrict__ input,\n",
        "  const FILTER_TYPE* __restrict__ filter\n",
        ") {\n",
        "  const int total_block_size = block_size_x * VECTOR_SIZE;\n",
        "  const int thread_x = threadIdx.x * VECTOR_SIZE;\n",
        "  const int global_x = blockIdx.x * total_block_size + thread_x;\n",
        "\n",
        "    // If we prefetch, read all required input elements into shared memory.\n",
        "#if PREFETCH_INPUT\n",
        "    const int shared_input_size = total_block_size + FILTER_SIZE - 1;\n",
        "    __shared__ INPUT_TYPE shared_input[shared_input_size];\n",
        "\n",
        "    for (int dx = 0; dx < shared_input_size; dx += total_block_size) {\n",
        "      if (global_x + dx < INPUT_SIZE && thread_x + dx < shared_input_size) {\n",
        "        auto items = kernel_float::read_aligned<VECTOR_SIZE>(\n",
        "          &input[global_x + dx]\n",
        "        );\n",
        "\n",
        "        kernel_float::write_aligned<VECTOR_SIZE>(\n",
        "          &shared_input[thread_x + dx], items\n",
        "        );\n",
        "      }\n",
        "    }\n",
        "\n",
        "    __syncthreads();\n",
        "#endif\n",
        "\n",
        "  if (global_x < OUTPUT_SIZE) {\n",
        "    kernel_float::vec<ACCUM_TYPE, VECTOR_SIZE> accumulate;\n",
        "\n",
        "#pragma unroll\n",
        "    for (int dx = 0; dx < FILTER_SIZE; dx++) {\n",
        "\n",
        "      // Read the input at dx, either from shared memory or global memory.\n",
        "#if PREFETCH_INPUT\n",
        "      auto items = kernel_float::read<VECTOR_SIZE>(&shared_input[thread_x + dx]);\n",
        "#else\n",
        "      auto items = kernel_float::read<VECTOR_SIZE>(&input[global_x + dx]);\n",
        "#endif\n",
        "\n",
        "      // Read the weight at dx\n",
        "      FILTER_TYPE weight = filter[dx];\n",
        "\n",
        "      // Add the product of the items multiplied by the weights\n",
        "      // `fma` is fused-mulitply-add, it performs \"weight * items + accumulate\"\n",
        "      accumulate = kernel_float::fma(\n",
        "          kernel_float::cast<ACCUM_TYPE>(weight),\n",
        "          kernel_float::cast<ACCUM_TYPE>(items),\n",
        "          accumulate\n",
        "      );\n",
        "    }\n",
        "\n",
        "    // Write the result to the output\n",
        "    kernel_float::write_aligned<VECTOR_SIZE>(&output[global_x], accumulate);\n",
        "  }\n",
        "}"
      ],
      "metadata": {
        "id": "S5vFF9vy4wvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem dimensions\n",
        "\n",
        "Now we generate inputs by defining the `filter_size` and the `problem_size`.\n",
        "\n",
        "The output signal will have size `problem_size`. A larger signal will take more processing time.\n",
        "\n",
        "The filter will have size `filter_size`. A larger filter will lead to more processing time, but might also result in better memory reuse due to the caches.\n",
        "\n",
        "Note that the size of the input signal is calculated automatically from the size of the output signal and the filter."
      ],
      "metadata": {
        "id": "x7a0o11HFFts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filter_size = 25\n",
        "problem_size = 50000000\n",
        "\n",
        "\n",
        "# Do not modify this cell below this line\n",
        "results = []\n",
        "input = np.random.default_rng(0).random(problem_size + filter_size - 1)\n",
        "\n",
        "filter = scipy.signal.windows.gaussian(filter_size, 2.0)\n",
        "filter /= np.sum(filter)\n",
        "\n",
        "output = np.zeros(problem_size)\n",
        "output_expect = np.convolve(input, filter, mode=\"valid\")\n",
        "\n",
        "plt.plot(np.arange(100), input[:100], label=\"input\")\n",
        "plt.plot(np.arange(100) + filter_size/2, output_expect[:100], label=\"expected output\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "C7wN0Eo045TW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Auto-tuning using Kernel Tuner\n",
        "\n",
        "The next cell shows a simple example of how to perform accuracy tuning using Kernel Tuner."
      ],
      "metadata": {
        "id": "TcFqJlOvFqbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kernel_tuner\n",
        "from kernel_tuner.accuracy import TunablePrecision, AccuracyObserver\n",
        "from kernel_tuner.observers.nvml import NVMLObserver\n",
        "\n",
        "# The tunable types. Currently, the code is only tuned for double precision.\n",
        "#\n",
        "# ================================================================\n",
        "# ✏️ Add you own data types here. You can use the following options: ✏️\n",
        "#\n",
        "# - \"double\"\n",
        "# - \"float\"\n",
        "# - \"half\"\n",
        "# - \"bfloat16\"\n",
        "# ================================================================\n",
        "#\n",
        "tune_params = dict()\n",
        "tune_params[\"OUTPUT_TYPE\"] = [\"double\"]\n",
        "tune_params[\"INPUT_TYPE\"] = [\"double\"]\n",
        "tune_params[\"FILTER_TYPE\"] = [\"double\"]\n",
        "tune_params[\"ACCUM_TYPE\"] = [\"double\"]\n",
        "\n",
        "# Other tunable parameters\n",
        "tune_params[\"block_size_x\"] = [128, 256]\n",
        "tune_params[\"loop_unroll_factor_filter\"] = [1, 5, 25]\n",
        "tune_params[\"VECTOR_SIZE\"] = [1, 2, 4]\n",
        "tune_params[\"PREFETCH_INPUT\"] = [0, 1]\n",
        "\n",
        "\n",
        "# Do not modify this cell below this line\n",
        "# Kernel arguments wrapped in `TunablePrecision`s\n",
        "args = [\n",
        "    TunablePrecision(\"OUTPUT_TYPE\", output),\n",
        "    TunablePrecision(\"INPUT_TYPE\", input),\n",
        "    TunablePrecision(\"FILTER_TYPE\", filter),\n",
        "]\n",
        "\n",
        "# The expected output\n",
        "answer = [output_expect, None, None]\n",
        "\n",
        "# Add observers to measure the accuracy and energy\n",
        "observers = [AccuracyObserver(\"RMSE\"), NVMLObserver([\"nvml_energy\", \"nvml_power\"])]\n",
        "\n",
        "# Tune for 2 minutes using random sampling\n",
        "strategy = \"random_sample\"\n",
        "strategy_options = dict(time_limit=2 * 60, fraction=1)\n",
        "\n",
        "# These compiler options are needed for compilation.\n",
        "compiler_options=[\n",
        "    \"--std=c++17\",\n",
        "    \"-I/content\",\n",
        "    \"-I/usr/local/cuda/include\",\n",
        "    f\"-DINPUT_SIZE={len(input)}\",\n",
        "    f\"-DOUTPUT_SIZE={len(output)}\",\n",
        "    f\"-DFILTER_SIZE={len(filter)}\",\n",
        "]\n",
        "\n",
        "# This restriction ensure we use either `half` or `bfloat16`, but not both\n",
        "restriction=lambda p: not (\"half\" in p.values() and \"bfloat16\" in p.values())\n",
        "\n",
        "# Run kernel tuner!\n",
        "results_batch, env = kernel_tuner.tune_kernel(\n",
        "    \"convolution\",\n",
        "    \"convolution.cu\",\n",
        "    problem_size,\n",
        "    args,\n",
        "    tune_params,\n",
        "    compiler_options=compiler_options,\n",
        "    answer=answer,\n",
        "    observers=observers,\n",
        "    lang=\"NVCUDA\",\n",
        "    strategy=strategy,\n",
        "    strategy_options=strategy_options,\n",
        "    iterations=15,\n",
        "    restrictions=restriction,\n",
        "    cache=\"convolution\"\n",
        ")\n",
        "\n",
        "results += results_batch"
      ],
      "metadata": {
        "id": "WsBGYMdn440C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analyzing the Results\n",
        "\n",
        "We can convert the results into a Pandas dataframe to visualize them using a table."
      ],
      "metadata": {
        "id": "4b0By89hTeg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to DataFrame and remove some unnecessary columns\n",
        "remove_columns = [\"times\", \"timestamp\", \"compile_time\", \"strategy_time\", \"benchmark_time\", \"verification_time\", \"framework_time\"]\n",
        "df = pandas.DataFrame(results).drop(columns=remove_columns).sort_values(\"time\")\n",
        "\n",
        "# Find speedup over lowest error, which should be double precision\n",
        "baseline_time = 10.605600 #df[\"time\"][df[\"error\"] == df[\"error\"].min()].min()\n",
        "df[\"speedup\"] = baseline_time / df[\"time\"]\n",
        "\n",
        "# Add column that indicates used data types\n",
        "type_columns = [\"OUTPUT_TYPE\", \"INPUT_TYPE\", \"FILTER_TYPE\", \"ACCUM_TYPE\"]\n",
        "df[\"data_types\"] = df[type_columns].apply(lambda row: \"+\".join(sorted(set(row))), axis=1)\n",
        "\n",
        "# Show table\n",
        "df"
      ],
      "metadata": {
        "id": "iiRicf3IPzQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can visualize the results on a performance vs error plot. We group them based on the data types used. For example, `half+double` means the data types used are a combination of `double` and `half`."
      ],
      "metadata": {
        "id": "LMFnVVVjT883"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.subplots(1, 2, figsize=(15, 8))\n",
        "\n",
        "# Speedup versus error\n",
        "plt.subplot(121)\n",
        "plt.title(\"Speedup versus error\")\n",
        "plt.yscale(\"log\")\n",
        "plt.xlabel(\"Speedup over double precision\")\n",
        "plt.ylabel(\"Error (RMSE)\")\n",
        "seaborn.scatterplot(df, x=\"speedup\", y=\"error\", hue=\"data_types\")\n",
        "\n",
        "# Energy versus error\n",
        "plt.subplot(122)\n",
        "plt.title(\"Energy versus error\")\n",
        "plt.yscale(\"log\")\n",
        "plt.xscale(\"log\")\n",
        "plt.xlabel(\"Energy usage (J)\")\n",
        "plt.ylabel(\"Error (RMSE)\")\n",
        "seaborn.scatterplot(df, x=\"nvml_energy\", y=\"error\", hue=\"data_types\")"
      ],
      "metadata": {
        "id": "Evi8NP-k8BZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pareto front\n",
        "\n",
        "Next we determine the _pareto front_. The pareto front is the set of all configurations that are _optimal_ in the sense that it is not possible to improve one property (the error or the run time or the energy) without sacrificing the other property.\n",
        "\n",
        "First, we print the pareto optimal points as a dataframe:"
      ],
      "metadata": {
        "id": "F26ZRkCkVXBZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function that checks if `row` dominates all other rows\n",
        "def dominates(row):\n",
        "    dominators = (row.time >= df[\"time\"]) & (row.error >= df[\"error\"]) & (row.nvml_energy >= df[\"nvml_energy\"]) & \\\n",
        "              ((row.time > df[\"time\"]) | (row.error > df[\"error\"]) | (row.nvml_energy > df[\"nvml_energy\"]))\n",
        "\n",
        "    return not dominators.any()\n",
        "\n",
        "# Extract the pareto front\n",
        "df[\"pareto\"] = df.apply(dominates, axis=1)\n",
        "\n",
        "df.loc[df[\"pareto\"]].sort_values(\"time\")"
      ],
      "metadata": {
        "id": "8K08sy4HHIF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also visualize the front on a speedup-error scatter plot:"
      ],
      "metadata": {
        "id": "5PBuwt6ZTCJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Speedup versus error\n",
        "plt.title(\"Speedup versus error\")\n",
        "plt.yscale(\"log\")\n",
        "plt.xlabel(\"Speedup over double precision\")\n",
        "plt.ylabel(\"Error (RMSE)\")\n",
        "\n",
        "seaborn.scatterplot(df.sort_values(\"pareto\"), x=\"speedup\", y=\"error\", hue=\"pareto\", palette=[\"lightgray\", \"red\"])"
      ],
      "metadata": {
        "id": "51bBIJVZN9wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Go experiment!\n",
        "Now, go back up to [cell 5](#scrollTo=WsBGYMdn440C), change some of the data types in `tune_params`, and execute the cell again. Afterwards, you can rerun the subsequent cells to see new results for different combinations of types.\n",
        "\n",
        "\n",
        "That's it! You've successfully completed the third hands-on!"
      ],
      "metadata": {
        "id": "eC4_qd91TOHV"
      }
    }
  ]
}